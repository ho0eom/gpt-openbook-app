# app/main.py
import os
import streamlit as st
import openai
import tiktoken
from rapidfuzz import fuzz
from typing import List, Tuple
import numpy as np

# â€” í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì„¸íŒ… â€”
openai.api_key = os.getenv("OPENAI_API_KEY")

# â€” ì„¤ì •ê°’ â€”
EMBEDDING_MODEL = "text-embedding-ada-002"
CHAT_MODEL = "gpt-3.5-turbo"
CHUNK_SIZE = 2000
CHUNK_OVERLAP = 500
TOP_K = 3
FUZZY_THRESHOLD = 50  # RapidFuzz ì ìˆ˜ í•„í„°

# â€” 1) í…ìŠ¤íŠ¸ ë¡œë“œ & ì²­í¬ ë¶„ë¦¬ â€”
@st.cache_data(show_spinner=False)
def load_and_chunk(path: str) -> Tuple[List[Tuple[int,str]], List[List[float]]]:
    full = open(path, "r", encoding="utf-8").read()
    # ê²¹ì¹¨(chunk) ê¸°ë°˜ ë¶„ë¦¬
    chunks = []
    pos = 0
    page = 1
    while pos < len(full):
        chunk = full[pos : pos + CHUNK_SIZE]
        chunks.append((page, chunk))
        pos += CHUNK_SIZE - CHUNK_OVERLAP
        page += 1
    # 2) ì„ë² ë”© í•œë²ˆë§Œ ê³„ì‚°
    embeddings = []
    for _, txt in chunks:
        resp = openai.embeddings.create(model=EMBEDDING_MODEL, input=txt)
        embeddings.append(resp["data"][0]["embedding"])
    return chunks, embeddings

# â€” 3) semantic + fuzzy ë‹¤ë‹¨ê³„ ê²€ìƒ‰ â€”
def retrieve(query: str,
             chunks: List[Tuple[int,str]],
             embeddings: List[List[float]]) -> Tuple[List[int], str]:
    # 3-1) ì§ˆë¬¸ ì„ë² ë”©
    resp = openai.embeddings.create(model=EMBEDDING_MODEL, input=query)
    q_emb = np.array(resp["data"][0]["embedding"])
    # 3-2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    sims = []
    for i, emb in enumerate(embeddings):
        sims.append((i, float(np.dot(q_emb, emb) / (np.linalg.norm(q_emb)*np.linalg.norm(emb)))))
    sims.sort(key=lambda x: x[1], reverse=True)
    # 3-3) top-k í›„ë³´ ì¤‘ fuzzy í•„í„° í›„ ì¬ì •ë ¬
    candidates = []
    for idx, _ in sims[:TOP_K]:
        score = fuzz.token_set_ratio(query, chunks[idx][1])
        if score >= FUZZY_THRESHOLD:
            candidates.append((idx, score))
    if not candidates:
        # fuzzy ëª» ë„˜ì€ ê²½ìš° semantic top1 ë§Œ ì‚¬ìš©
        candidates = [(sims[0][0], fuzz.token_set_ratio(query, chunks[sims[0][0]][1]))]
    # ìµœì¢… 1ìˆœìœ„
    best_idx = max(candidates, key=lambda x: x[1])[0]
    # top-k ì „ì²´ í•©ì³ì„œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
    top_idxs = [i for i, _ in sims[:TOP_K]]
    combined = "\n\n---\n\n".join(chunks[i][1] for i in top_idxs)
    pages = [chunks[i][0] for i in top_idxs]
    return pages, combined

# â€” 4) GPTì—ê²Œ ì§ˆë¬¸ ë˜ì§€ê¸° â€”
def ask_gpt(query: str, pages: List[int], context: str) -> str:
    system = (
        "You are an assistant that answers questions *only* from the provided context. "
        "If the answer isnâ€™t in the context, say you couldnâ€™t find it."
    )
    user = (
        f"[=== CONTEXT START (pages {pages}) ===]\n"
        f"{context}\n"
        f"[=== CONTEXT END ===]\n\n"
        f"Question: {query}\n\n"
        "Please answer in one sentence and **quote** the exact sentence from the context as evidence."
    )
    resp = openai.chat.completions.create(
        model=CHAT_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        temperature=0.0,
        max_tokens=512,
    )
    return resp.choices[0].message.content.strip()

# â€” Streamlit UI â€”
st.title("ğŸ“˜ ì˜¤í”ˆë¶ Q&A (ì„ë² ë”©+í¼ì§€ ê²€ìƒ‰)")
st.write("ì˜¤íƒ€Â·ë™ì˜ì–´ OK Â· ì˜¤ì§ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ë‹µê³¼ ê·¼ê±° ì¸ìš©")

# ì´ˆê¸° ë¡œë“œ
chunks, embeddings = load_and_chunk("pdf_text/your_pdf.txt")

query = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if query:
    with st.spinner("ê²€ìƒ‰ ì¤‘â€¦"):
        pages, ctx = retrieve(query, chunks, embeddings)
    with st.spinner("AIê°€ ë‹µë³€ ìƒì„± ì¤‘â€¦"):
        answer = ask_gpt(query, pages, ctx[:5000])
    st.subheader("âœ… ë‹µë³€")
    st.write(answer)
