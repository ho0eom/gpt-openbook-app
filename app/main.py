import os
import streamlit as st
import openai
import tiktoken
import numpy as np
from rapidfuzz import fuzz

# â€” í™˜ê²½ ë³€ìˆ˜ì—ì„œ í‚¤ ì½ê¸° â€”
openai.api_key = os.getenv("OPENAI_API_KEY")

# â€” ì„¤ì •ê°’ â€”
EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_MODEL      = "gpt-3.5-turbo"
CHUNK_SIZE      = 1000   # ê¸€ì ë‹¨ìœ„
CHUNK_OVERLAP   = 200
TOP_K           = 5
FUZZY_THRESHOLD = 60     # í¼ì§€ë¹„êµ ì»·ì˜¤í”„

# â€” í…ìŠ¤íŠ¸ ë¡œë“œ + ì²­í¬í™” + ì˜¤ë²„ë© â€”
@st.cache_data(show_spinner=False)
def load_and_chunk(path: str):
    text = open(path, "r", encoding="utf-8").read()
    chunks = []
    i = 0
    while i < len(text):
        chunk = text[i : i + CHUNK_SIZE]
        chunks.append(chunk)
        i += CHUNK_SIZE - CHUNK_OVERLAP
    return chunks

# â€” ì„ë² ë”© ê³„ì‚° â€”
@st.cache_data(show_spinner=False)
def embed_texts(texts: list[str]) -> np.ndarray:
    embeddings = []
    for txt in texts:
        resp = openai.embeddings.create(model=EMBEDDING_MODEL, input=txt)
        embeddings.append(resp["data"][0]["embedding"])
    return np.array(embeddings)

# â€” ì§ˆë¬¸ ì„ë² ë”© + ìƒìœ„ K ì„ íƒ + í¼ì§€ í•„í„°ë§ â€”
def retrieve_best_chunk(question: str, chunks: list[str], embeddings: np.ndarray):
    # ì§ˆë¬¸ ì„ë² ë”©
    q_emb = openai.embeddings.create(model=EMBEDDING_MODEL, input=question)["data"][0]["embedding"]
    q_vec = np.array(q_emb)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    sims = (embeddings @ q_vec) / (
        np.linalg.norm(embeddings, axis=1) * np.linalg.norm(q_vec) + 1e-8
    )
    idxs = np.argsort(-sims)[:TOP_K]

    # í¼ì§€ë§¤ì¹­ìœ¼ë¡œ 2ì°¨ í•„í„°
    filtered = [(i, chunks[i]) for i in idxs if fuzz.partial_ratio(question, chunks[i]) >= FUZZY_THRESHOLD]
    if not filtered:
        filtered = [(i, chunks[i]) for i in idxs]  # ì»·ì˜¤í”„ ë¯¸ë‹¬ ì‹œ ìƒìœ„Kë¡œ ëŒ€ì²´

    return filtered[0]  # (index, text)

# â€” GPTì—ê²Œ ì§ˆë¬¸ & ê·¼ê±° ì¸ìš© ì§€ì‹œ â€”
def ask_gpt(question: str, context: str):
    prompt = f"""
ì•„ë˜ëŠ” ë¬¸ì„œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. **ì˜¤ì§ ì´ ë¬¸ë§¥ ì•ˆì—ì„œë§Œ** ì§ˆë¬¸ì— ë‹µí•˜ê³ , **ë‹µì˜ ê·¼ê±° ë¬¸ì¥ì€ ë¬¸ë§¥ì—ì„œ ë˜‘ê°™ì´ ì¸ìš©**í•˜ì„¸ìš”.


â“ ì§ˆë¬¸: {question}

â–¶ï¸ ë‹µë³€ í˜•ì‹:
1) ë‹µë³€:
2) ê·¼ê±° ì¸ìš©(ë¬¸ì¥ ê·¸ëŒ€ë¡œ):
"""
    resp = openai.chat.completions.create(
        model=CHAT_MODEL,
        messages=[{"role":"user","content":prompt}],
        temperature=0.0,
        max_tokens=500,
    )
    return resp.choices[0].message.content.strip()

# â€” Streamlit UI â€”
st.title("ğŸ“˜ ì˜¤í”ˆë¶ Q&A (ì„ë² ë”©+í¼ì§€ ê²€ìƒ‰)")
st.write("ì˜¤íƒ€Â·ë™ì˜ì–´ OK Â· ì˜¤ì§ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ë‹µê³¼ ê·¼ê±° ì¸ìš©")

question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if question:
    with st.spinner("ë¬¸ì„œ ë¡œë”© ë° ì„ë² ë”© ì¤‘..."):
        chunks     = load_and_chunk("pdf_text/your_pdf.txt")
        embeddings = embed_texts(chunks)

    idx, best_chunk = retrieve_best_chunk(question, chunks, embeddings)
    st.markdown(f"**â–¶ï¸ ì„ íƒëœ ì²­í¬ ì¸ë±ìŠ¤:** {idx}")

    with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„± ì¤‘..."):
        answer = ask_gpt(question, best_chunk)

    st.subheader("âœ… GPTì˜ ë‹µë³€")
    st.write(answer)
