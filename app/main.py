import os
import streamlit as st
from openai import OpenAI
from rapidfuzz import fuzz
from typing import List, Tuple

# â€” í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸° â€”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ì„¤ì •
EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-3.5-turbo"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
TOP_K = 5

# â€” í…ìŠ¤íŠ¸ ë¡œë“œ ë° ì²­í¬ ë¶„ë¦¬ + ì„ë² ë”© ìƒì„± â€”
@st.cache_data
def load_and_chunk(path: str) -> Tuple[List[str], List[List[float]]]:
    text = open(path, "r", encoding="utf-8").read()
    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í¬
    chunks = []
    for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):
        chunk = text[i : i + CHUNK_SIZE]
        chunks.append(chunk)
    # ì„ë² ë”©
    embeddings = []
    for txt in chunks:
        resp = client.embeddings.create(model=EMBEDDING_MODEL, input=txt)
        # --- ì—¬ê¸°ë§Œ ë³€ê²½ ---
        embeddings.append(resp.data[0].embedding)
    return chunks, embeddings

# â€” ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± & ìœ ì‚¬ë„ ìƒìœ„ Kê°œ ì°¾ê¸° + í¼ì§€ ë§¤ì¹­ í•„í„°ë§ ---
@st.cache_data
def semantic_search(query: str, chunks, embeddings) -> List[Tuple[int,str]]:
    q_emb = client.embeddings.create(model=EMBEDDING_MODEL, input=query).data[0].embedding
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    from numpy import dot
    from numpy.linalg import norm
    scores = [
        dot(q_emb, emb) / (norm(q_emb)*norm(emb)+1e-8)
        for emb in embeddings
    ]
    # ìƒìœ„ K ì¸ë±ìŠ¤
    top_idxs = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:TOP_K]
    # í¼ì§€ ë§¤ì¹­ìœ¼ë¡œ ì¶”ê°€ í›„ë³´ ê±¸ëŸ¬ë‚´ê¸°
    results = []
    for i in top_idxs:
        score = fuzz.partial_ratio(query, chunks[i])
        if score > 50:  # ìœ ì‚¬ë„ ì„ê³„ê°’
            results.append((i, chunks[i]))
    return results

# â€” GPTì— ì§ˆë¬¸ ë˜ì§€ê¸° (ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œë§Œ ë‹µí•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ê³ ì •) â€”
def ask_gpt(question: str, contexts: List[Tuple[int,str]]) -> str:
    prompt = "ì•„ë˜ ì²­í¬ë“¤ë§Œ ë³´ê³ , ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ê° ë‹µ ë’¤ì— ë°˜ë“œì‹œ ì¸ìš© ë¬¸ì¥(ì²­í¬ì—ì„œ ê·¸ëŒ€ë¡œ ë³µë¶™í•œ ë¬¸ì¥)ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
    for idx, chunk in contexts:
        prompt += f"[ì²­í¬ {idx}]\n{chunk}\n\n"
    prompt += f"ì§ˆë¬¸: {question}\n\në‹µë³€:"

    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=[{"role":"user","content":prompt}],
        temperature=0
    )
    return resp.choices[0].message.content.strip()

# â€” Streamlit UI â€”
st.title("ğŸ“˜ ì˜¤í”ˆë¶ Q&A (ì„ë² ë”©+í¼ì§€ ê²€ìƒ‰)")
st.write("ì˜¤íƒ€Â·ë™ì˜ì–´ OK Â· ì˜¤ì§ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ë‹µê³¼ ê·¼ê±° ì¸ìš©")

question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if question:
    chunks, embeddings = load_and_chunk("pdf_text/your_pdf.txt")
    candidates = semantic_search(question, chunks, embeddings)
    if not candidates:
        st.warning("ê´€ë ¨ ë¬¸ë§¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        with st.spinner("AIê°€ ë‹µë³€ì„ ë§Œë“¤ê³  ìˆì–´ìš”..."):
            answer = ask_gpt(question, candidates)
        st.subheader("âœ… ë‹µë³€ ë° ì¸ìš© ê·¼ê±°")
        st.write(answer)
