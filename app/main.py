import os
import streamlit as st
from rapidfuzz import fuzz
from openai import OpenAI
from openai import OpenAIError

# â€” í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” â€”
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

client = OpenAI(api_key=api_key)

# â€” ì„¤ì • â€”
EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-3.5-turbo"

# â€” PDF/TXT í…ìŠ¤íŠ¸ ë¡œë“œ â€”
@st.cache_data
def load_text(path: str = "pdf_text/your_pdf.txt") -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

# â€” í…ìŠ¤íŠ¸ë¥¼ ì²­í¬(=í˜ì´ì§€) ë‹¨ìœ„ë¡œ ë¶„ë¦¬ â€”
@st.cache_data
def chunk_text(full_text: str, chunk_size: int = 1000, overlap: int = 200):
    chunks = []
    start = 0
    while start < len(full_text):
        end = min(start + chunk_size, len(full_text))
        chunks.append(full_text[start:end])
        start += chunk_size - overlap
    return chunks

# â€” ì²­í¬ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ â€”
@st.cache_data
def embed_texts(chunks: list[str]) -> list[list[float]]:
    embeddings = []
    for txt in chunks:
        try:
            resp = client.embeddings.create(model=EMBEDDING_MODEL, input=txt)
            # resp.data[0].embedding ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤
            embeddings.append(resp.data[0].embedding)
        except OpenAIError as e:
            st.error(f"ì„ë² ë”© ì¤‘ ì˜¤ë¥˜: {e}")
            embeddings.append([0.0])  # ì‹¤íŒ¨ì‹œ ë”ë¯¸
    return embeddings

# â€” ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ ì²­í¬ ì°¾ê¸° â€”
def find_best_chunk(query: str, chunks: list[str], embeddings: list[list[float]]) -> int:
    # ì¿¼ë¦¬ë„ ì„ë² ë”©í•´ì„œ ë¹„êµí•˜ê±°ë‚˜, í¼ì§€ ë§¤ì¹­ìœ¼ë¡œ ë‹¨ìˆœ ë¹„êµ
    scores = []
    for chunk in chunks:
        # RapidFuzz í¼ì§€ ì ìˆ˜
        score = fuzz.token_sort_ratio(query, chunk)
        scores.append(score)
    return max(range(len(chunks)), key=lambda i: scores[i])

# â€” GPTì— ì§ˆë¬¸ ë˜ì§€ê¸° â€”
def ask_gpt(question: str, context: str) -> str:
    prompt = (
        "ì•„ë˜ ë¬¸ë§¥ë§Œ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ê³ , ë‹µì˜ ê·¼ê±° ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì¸ìš©í•´ì£¼ì„¸ìš”.\n\n"
        f"ë¬¸ë§¥:\n```{context}```\n\n"
        f"ì§ˆë¬¸: {question}\n"
    )
    try:
        resp = client.chat.completions.create(
            model=CHAT_MODEL,
            messages=[{"role": "user", "content": prompt}],
        )
        return resp.choices[0].message.content.strip()
    except OpenAIError as e:
        return f"ChatCompletion ì˜¤ë¥˜: {e}"

# â€” Streamlit UI â€”
st.title("ğŸ“˜ ì˜¤í”ˆë¶ Q&A (ì„ë² ë”©+í¼ì§€ ê²€ìƒ‰)")
st.write("ì˜¤íƒ€Â·ë™ì˜ì–´ OK Â· ì˜¤ì§ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ë‹µê³¼ ê·¼ê±° ì¸ìš©")

question = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if question:
    full = load_text()
    chunks = chunk_text(full)
    embeddings = embed_texts(chunks)

    idx = find_best_chunk(question, chunks, embeddings)
    context = chunks[idx]

    with st.spinner(f"ì²­í¬ #{idx+1}ì—ì„œ ë‹µë³€ ìƒì„± ì¤‘â€¦"):
        answer = ask_gpt(question, context[:1500])

    st.subheader("âœ… GPTì˜ ë‹µë³€")
    st.write(answer)
